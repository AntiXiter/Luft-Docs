# routes/ia.py
import os
import re
import chromadb
import google.generativeai as genai
import groq
import openai
from flask import Blueprint, render_template, request, jsonify
from dotenv import load_dotenv

ia_bp = Blueprint('ia_bp', __name__)
load_dotenv()

# --- L√ìGICA PARA DESCOBRIR M√ìDULOS AUTOMATICAMENTE ---
def get_available_modules():
    """Verifica o diret√≥rio data/modules e retorna uma lista com os nomes dos m√≥dulos."""
    try:
        modules_path = os.path.join("data", "modules")
        if not os.path.exists(modules_path):
            print("AVISO: Diret√≥rio 'data/modules' n√£o encontrado. O filtro din√¢mico n√£o funcionar√°.")
            return []
        available = [d for d in os.listdir(modules_path) if os.path.isdir(os.path.join(modules_path, d))]
        print(f"M√≥dulos descobertos automaticamente: {available}")
        return available
    except Exception as e:
        print(f"ERRO CR√çTICO ao descobrir m√≥dulos: {e}")
        return []

AVAILABLE_MODULES = get_available_modules()
# -------------------------------------------------------------

# --- CONFIGURA√á√ÉO PARA TODOS OS MODELOS ---
try:
    # Gemini
    gemini_api_key = os.getenv("GEMINI_API_KEY")
    if not gemini_api_key: raise ValueError("Chave da API do Gemini n√£o encontrada.")
    genai.configure(api_key=gemini_api_key)
    embedding_model = 'models/text-embedding-004'
    gemini_generation_model = genai.GenerativeModel('gemini-1.5-flash-latest')

    # Groq
    groq_api_key = os.getenv("GROQ_API_KEY")
    if not groq_api_key: raise ValueError("Chave da API do Groq n√£o encontrada.")
    groq_client = groq.Groq(api_key=groq_api_key)

    # OpenAI (mesmo que desabilitado no frontend, o backend est√° pronto)
    openai_api_key = os.getenv("OPENAI_API_KEY")
    openai_client = openai.OpenAI(api_key=openai_api_key) if openai_api_key else None
    
    # ChromaDB
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
    collection = chroma_client.get_collection(name="luftdocs_collection")
    
    print("M√≥dulo de IA: Modelos (OpenAI, Groq, Gemini) e DB Vetorial carregados com sucesso.")

except Exception as e:
    print(f"ERRO CR√çTICO no setup da IA: {e}")
    openai_client = None; groq_client = None; gemini_generation_model = None; collection = None

@ia_bp.route('/test_ia')
def test_ia_page():
    return render_template('test_ia.html')

@ia_bp.route('/api/ask_llm', methods=['POST'])
def ask_llm_api():
    if not all([groq_client, gemini_generation_model, collection]):
        return jsonify({"error": "Componentes da IA n√£o est√£o configurados."}), 500

    data = request.get_json()
    if not data or 'user_question' not in data:
        return jsonify({"error": "Requisi√ß√£o inv√°lida."}), 400

    user_question = data['user_question']
    selected_model = data.get('selected_model', 'groq-70b') 

    # --- ETAPA 1: BUSCA DE CONTEXTO COM FILTRO DIN√ÇMICO ---
    question_embedding = genai.embed_content(model=embedding_model, content=user_question, task_type="RETRIEVAL_QUERY")['embedding']
    query_filter = {}
    question_lower = user_question.lower()
    found_modules = [module for module in AVAILABLE_MODULES if module.replace('-', ' ') in question_lower]
    if found_modules:
        query_filter = {"$or": [{"module": name} for name in found_modules]} if len(found_modules) > 1 else {"module": found_modules[0]}
        print(f"Busca filtrada DIN√ÇMICA ativada. Filtro: {query_filter}")
    else:
        print("Nenhum m√≥dulo espec√≠fico mencionado na pergunta, realizando busca geral.")
    relevant_chunks = collection.query(query_embeddings=[question_embedding], n_results=5, where=query_filter if query_filter else None)
    if not relevant_chunks['documents'][0] and query_filter:
        print(f"Busca filtrada por {query_filter} n√£o retornou resultados. Tentando busca geral como fallback.")
        relevant_chunks = collection.query(query_embeddings=[question_embedding], n_results=5)
    context = "\n---\n".join(relevant_chunks['documents'][0])
    sources = [meta['source'] for meta in relevant_chunks['metadatas'][0]]
    unique_sources = sorted(list(set(sources)))
    
    answer = ""
    try:
        # --- ETAPA 2: GERA√á√ÉO DA RESPOSTA COM PROMPTS COMPLETOS ---
        system_prompt = """Voc√™ √© a 'Lia', a assistente de conhecimento gente boa da LuftDocs. Sua miss√£o √© ajudar seus colegas de equipe a encontrar informa√ß√µes de forma clara, amig√°vel e descomplicada. Pense e responda como se fosse um colega de trabalho brasileiro: prestativo, um pouco informal e que vai direto ao ponto sem ser rob√≥tico. Use um toque de "malemol√™ncia" e bom humor.

**Suas regras de ouro para responder:**

1.  **Personalidade:**
    * **Sauda√ß√£o:** Comece sempre com um "Opa, vamos l√°!", "Beleza! Encontrei isso aqui pra voc√™:" ou algo nesse estilo descontra√≠do.
    * **Tom:** Mantenha um tom conversacional e prestativo. Use emojis para dar um toque de personalidade quando fizer sentido (üòâ, üëç).
    * **Despedida:** Termine com algo amig√°vel como "Qualquer outra d√∫vida, √© s√≥ chamar!" ou "Espero que ajude!".

2.  **Estrutura da Resposta (Use Markdown):**
    * **S√≠ntese:** Comece com um par√°grafo curto resumindo a resposta direta para a pergunta.
    * **T√≠tulos e Destaques:** Use t√≠tulos com `##` para separar se√ß√µes e negrito (`**palavra**`) para destacar termos importantes.
    * **Listas:** Se for um processo com passos, use listas com marcadores (`*`).
    * **REGRA CR√çTICA PARA IMAGENS:** Se o contexto contiver um caminho para uma imagem (ex: `/data/img/tela/foto.png`), √â SUA OBRIGA√á√ÉO **INCLUIR O CAMINHO EXATO DO ARQUIVO COMO TEXTO** na sua resposta, perto da descri√ß√£o da imagem. Deixe o caminho do arquivo vis√≠vel no texto.

3.  **Foco e Honestidade:**
    * Responda usando **apenas** o contexto fornecido.
    * Se a informa√ß√£o n√£o estiver l√°, seja honesta de forma amig√°vel. Diga algo como: "Olha, dei uma boa fu√ßada aqui nos documentos, mas n√£o achei os detalhes sobre isso. üôÅ"

Lembre-se: seja a colega de trabalho que todo mundo gostaria de ter para tirar uma d√∫vida!"""
        human_prompt = f"**Contexto da Documenta√ß√£o:**\n{context}\n\n**Pergunta do Usu√°rio:** \"{user_question}\""

        if selected_model == 'groq-70b':
            print("Gerando resposta com Groq (Llama 3 70b - Poderoso)...")
            chat_completion = groq_client.chat.completions.create(messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": human_prompt}], model="llama3-70b-8192")
            answer = chat_completion.choices[0].message.content

        elif selected_model == 'groq-8b':
            print("Gerando resposta com Groq (Llama 3 8b - R√°pido)...")
            chat_completion = groq_client.chat.completions.create(messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": human_prompt}], model="llama3-8b-8192")
            answer = chat_completion.choices[0].message.content
        
        elif selected_model == 'openai' and openai_client:
            print("Gerando resposta com OpenAI (GPT-4o)...")
            chat_completion = openai_client.chat.completions.create(messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": human_prompt}], model="gpt-4o")
            answer = chat_completion.choices[0].message.content

        elif selected_model == 'gemini':
            print("Gerando resposta com Gemini (1.5 Flash)...")
            gemini_prompt = f"{system_prompt}\n---\n{human_prompt}"
            response = gemini_generation_model.generate_content(gemini_prompt)
            answer = response.text
        
        else:
            return jsonify({"error": f"Modelo '{selected_model}' inv√°lido ou n√£o configurado."}), 400

        # --- ETAPA 3: P√ìS-PROCESSAMENTO √Ä PROVA DE FALHAS PARA IMAGENS ---
        def force_image_formatting(text):
            processed_text = text.replace('/data/img/', '/data/img/')
            image_pattern = r'(/data/img/[^\s\)<]+\.(png|jpg|jpeg|gif))'
            replacement_format = r'\n\n![Imagem do Documento](\1)\n\n'
            final_text = re.sub(image_pattern, replacement_format, processed_text)
            return final_text
        final_answer = force_image_formatting(answer)

        return jsonify({"answer": final_answer, "context_files": unique_sources})

    except Exception as e:
        return jsonify({"error": f"Erro ao gerar resposta com {selected_model}: {e}"}), 500